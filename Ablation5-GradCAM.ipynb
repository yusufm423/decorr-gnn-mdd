{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from Utils import compute_KNN_graph, fisher_z_transform, create_graph, to_tensor, create_graph_sliding_window\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.nn import GATConv, ChebConv\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import os.path as osp\n",
    "import csv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=89\n",
    "atlas_name= \"AAL\"\n",
    "dataset_name = \"MDDvHC\"\n",
    "\n",
    "\n",
    "if atlas_name == \"AAL\":\n",
    "    start = 0\n",
    "    end = 116\n",
    "elif atlas_name == \"Craddock\":\n",
    "    start = 228\n",
    "    end = 428\n",
    "elif atlas_name == \"Dosenbach\":\n",
    "    start = 1408\n",
    "    end = 1568\n",
    "else:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f90effb3fb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to set seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Environment variables for reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Set your seed\n",
    "set_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize(matrix):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_matrix = scaler.fit_transform(matrix)\n",
    "    return normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sliding window\n",
    "def create_graph_sliding_window_demographics(X, D, Y, start, end, region=True):\n",
    "    S = 30 # Sliding Step\n",
    "    T = 60 # Window Size\n",
    "\n",
    "    X_adjgraph=[]\n",
    "    X_featgraph = []\n",
    "    Y_list = []\n",
    "    num_samples_per_subject = []\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        #select rows according to atlas\n",
    "        # bold_matrix = X[i][start:end,:]\n",
    "        bold_matrix = X[i]\n",
    "        n = bold_matrix.shape[0]\n",
    "        demog = D[i]\n",
    "        # Unsqueeze and expand the demog array to create a 116x4 matrix\n",
    "        demog_expanded = np.expand_dims(demog, axis=0)  # Shape (1, 4)\n",
    "        demog_expanded = np.repeat(demog_expanded, n, axis=0)  # Shape (116, 4)\n",
    "        \n",
    "        temp_y = Y[i]\n",
    "\n",
    "        num_rows, num_cols = bold_matrix.shape\n",
    "        num_samples = 0\n",
    "        for start_idx in range(0, num_cols - T + 1, S):\n",
    "            end_idx = start_idx + T\n",
    "            if end_idx <= num_cols:\n",
    "                \n",
    "                if region == True:\n",
    "                    window_data =  bold_matrix[:, start_idx:end_idx]    #RxR\n",
    "                else:\n",
    "                    window_data =  np.transpose(bold_matrix[:, start_idx:end_idx])  #TxT\n",
    "\n",
    "                window_data1 = np.corrcoef(window_data)\n",
    "                correlation_matrix_fisher = fisher_z_transform(window_data1)\n",
    "                correlation_matrix_fisher = np.around(correlation_matrix_fisher, 8)     #upto 8 decimal points\n",
    "                # Concatenate the demog_expanded with correlation_matrix_fisher along the column\n",
    "                result_matrix = np.concatenate((correlation_matrix_fisher, demog_expanded), axis=1)\n",
    "\n",
    "                \n",
    "                knn_graph = compute_KNN_graph(correlation_matrix_fisher)\n",
    "\n",
    "                if region == True:\n",
    "                    X_featgraph.append(result_matrix)\n",
    "                else:\n",
    "                    X_featgraph.append(window_data)\n",
    "                    \n",
    "                X_adjgraph.append(knn_graph)\n",
    "                Y_list.append(temp_y)\n",
    "                num_samples = num_samples+1\n",
    "            \n",
    "        num_samples_per_subject.append(num_samples)\n",
    "\n",
    "\n",
    "    return X_featgraph, X_adjgraph, Y_list, num_samples_per_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_demographics(X, D, Y, start, end, region=True):\n",
    "    X_adjgraph=[]\n",
    "    X_featgraph = []\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        if region == True:\n",
    "            # bold_matrix = X[i][start:end,:] #RxR\n",
    "            bold_matrix = X[i]\n",
    "        else:\n",
    "            bold_matrix = np.transpose(X[i][start:end,:]) #TxT\n",
    "\n",
    "        n= bold_matrix.shape[0]\n",
    "        # Unsqueeze and expand the demog array to create a 116x4 matrix\n",
    "        demog_expanded = np.expand_dims(D[i], axis=0)  # Shape (1, 4)\n",
    "        demog_expanded = np.repeat(demog_expanded, n, axis=0)  # Shape (116, 4)\n",
    "        \n",
    "        window_data1 = np.corrcoef(bold_matrix)\n",
    "        correlation_matrix_fisher = fisher_z_transform(window_data1)\n",
    "        correlation_matrix_fisher = np.around(correlation_matrix_fisher, 8)\n",
    "        knn_graph = compute_KNN_graph(correlation_matrix_fisher)\n",
    "        # Concatenate the demog_expanded with correlation_matrix_fisher along the column\n",
    "        result_matrix = np.concatenate((correlation_matrix_fisher, demog_expanded), axis=1)\n",
    "\n",
    "        if region == True:\n",
    "            X_featgraph.append(result_matrix)\n",
    "        else:\n",
    "            X_featgraph.append(bold_matrix)\n",
    "            \n",
    "        X_adjgraph.append(knn_graph)\n",
    "\n",
    "    return X_featgraph, X_adjgraph, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv, global_mean_pool,GATConv\n",
    "\n",
    "\n",
    "\n",
    "class SkipConnModel(nn.Module):\n",
    "    def __init__(self, num_features_R, num_classes, k_order, dropout_prob=0.5):\n",
    "        super(SkipConnModel, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        self.num_layers = 6\n",
    "        self.convs = nn.ModuleList()  # List for model 1\n",
    "        self.bns = nn.ModuleList()    # BatchNorm for model 1\n",
    "        \n",
    "        # Define the first GCN model\n",
    "        self.convs.append(ChebConv(num_features_R, 128, K=3, normalization='sym'))\n",
    "        self.bns.append(nn.BatchNorm1d(128))\n",
    "        \n",
    "        self.convs.append(ChebConv(128, 128, K=3, normalization='sym'))\n",
    "        self.bns.append(nn.BatchNorm1d(128))\n",
    "\n",
    "        self.convs.append(ChebConv(128, 128, K=3, normalization='sym'))\n",
    "        self.bns.append(nn.BatchNorm1d(128))\n",
    "        \n",
    "        # self.convs.append(ChebConv(128, 128, K=3, normalization='sym'))\n",
    "        # self.bns.append(nn.BatchNorm1d(128))\n",
    "        \n",
    "        self.out_fc = nn.Linear(128, num_classes)\n",
    "        self.weights = torch.nn.Parameter(torch.randn(len(self.convs)))\n",
    "        # Add the decov weight as a parameter\n",
    "        #self.current_decov_weight = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        self.out_fc.reset_parameters()\n",
    "        torch.nn.init.normal_(self.weights)\n",
    "\n",
    "    def forward(self, data_R):\n",
    "        # First GCN model\n",
    "        x1, edge_index1, edge_attr1 = data_R.x, data_R.edge_index, data_R.edge_attr\n",
    "        batch1 = data_R.batch\n",
    "        \n",
    "        layer_out1 = []  # List to store the outputs of model 1\n",
    "        x1 = self.convs[0](x1, edge_index1, edge_attr1)\n",
    "        x1 = self.bns[0](x1)\n",
    "        x1 = F.relu(x1, inplace=True)\n",
    "        layer_out1.append(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout_prob, training=self.training)\n",
    "        \n",
    "                          \n",
    "        x1 = self.convs[1](x1, edge_index1, edge_attr1)\n",
    "        x1 = self.bns[1](x1)\n",
    "        x1 = F.relu(x1, inplace=True)\n",
    "        x1 = x1 + 0.8 * layer_out1[0]\n",
    "        layer_out1.append(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout_prob, training=self.training) \n",
    "        \n",
    "                          \n",
    "        x1 = self.convs[2](x1, edge_index1, edge_attr1)\n",
    "        x1 = self.bns[2](x1)\n",
    "        x1 = F.relu(x1, inplace=True)\n",
    "        x1 = x1 + 0.8 * layer_out1[1]\n",
    "        layer_out1.append(x1)\n",
    "        # x1 = F.dropout(x1, p=self.dropout_prob, training=self.training) \n",
    "\n",
    "\n",
    "        # x1 = self.convs[3](x1, edge_index1, edge_attr1)\n",
    "        # x1 = self.bns[3](x1)\n",
    "        # x1 = F.relu(x1, inplace=True)\n",
    "        # x1 = x1 + 1 * layer_out1[2]\n",
    "        # layer_out1.append(x1)\n",
    "        #x1 = F.dropout(x1, p=self.dropout_prob, training=self.training) \n",
    "        \n",
    "\n",
    "        # Apply softmax to the weights\n",
    "        weight = F.softmax(self.weights, dim=0)\n",
    "        \n",
    "        # Multiply each layer output by its corresponding weight\n",
    "        weighted_outs = [layer_out1[i] * weight[i] for i in range(len(layer_out1))]\n",
    "        \n",
    "        # Sum the weighted outputs to get the final embedding\n",
    "        emb = sum(weighted_outs)\n",
    "        \n",
    "        # Apply global mean pooling on the summed weighted outputs\n",
    "        pooled_emb = global_mean_pool(emb, batch1)\n",
    "\n",
    "        # Pass the final pooled embedding through the linear layer\n",
    "        x = self.out_fc(pooled_emb)\n",
    "        \n",
    "        return x, pooled_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def DECOV(embeddings):\n",
    "    # Transpose the embeddings\n",
    "    embeddings_t = embeddings.T  # Now it's 32 x batch\n",
    "\n",
    "    # Compute the covariance matrix along the columns\n",
    "    C = torch.cov(embeddings_t)  # Only works with PyTorch 1.9.0 or later\n",
    "\n",
    "    # Frobenius norm of the covariance matrix\n",
    "    C_fro_norm = torch.norm(C, p='fro')\n",
    "\n",
    "    # Diagonal elements of the covariance matrix\n",
    "    diag_elements = torch.diag(C,0)\n",
    "\n",
    "    # L2 norm of the diagonal elements\n",
    "    C2_l2norm_diag = torch.norm(diag_elements)\n",
    "\n",
    "    # DeCov loss\n",
    "    L_DECOV = (C_fro_norm ** 2) - (C2_l2norm_diag ** 2)\n",
    "\n",
    "    return L_DECOV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCN_train(loader):\n",
    "    model.train()\n",
    "\n",
    "    pred = []\n",
    "    label = []\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, pooled = model(data)  # Get both output and embeddings\n",
    "\n",
    "         #Ensure pooled_emb is on the correct device for DECOV\n",
    "        output = output.to('cpu')\n",
    "        loss_decov = DECOV(output)\n",
    "        loss_decov = loss_decov.to(device)  # Move loss_decov back to the GPU if needed\n",
    "\n",
    "        output = output.to(device)  # Move back to the original device if needed\n",
    "        \n",
    "        loss1 = func.cross_entropy(output, data.y) \n",
    "        #print(\"\\n\\nThe DeCov loss is\", loss_decov)\n",
    "       # print(\"\\n\\nThe Cross Entropy loss is\", loss1)\n",
    "        #print(\"\\n\\n decov weight\",  model.current_decov_weight)\n",
    "\n",
    "        loss = loss1 + 1e-12*loss_decov\n",
    "        #loss=loss1\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred.append(func.softmax(output, dim=1).max(dim=1)[1])\n",
    "        label.append(data.y)\n",
    "   \n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    epoch_sen = tp / (tp + fn)\n",
    "    epoch_spe = tn / (tn + fp)\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return epoch_sen, epoch_spe, epoch_acc, f1, loss_all/len(loader)\n",
    "\n",
    "\n",
    "def GCN_test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    scores = []\n",
    "    label = []\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output , pooled= model(data)\n",
    "        \n",
    "        output = output.to('cpu')\n",
    "        loss_decov = DECOV(output)\n",
    "        loss_decov = loss_decov.to(device)  # Move loss_decov back to the GPU if needed\n",
    "\n",
    "        output = output.to(device)  # Move back to the original device if needed\n",
    "        \n",
    "        loss1 = func.cross_entropy(output, data.y)\n",
    "        loss = loss1 + 1e-12*loss_decov\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "   \n",
    "        softmax_output = func.softmax(output, dim=1)\n",
    "        scores.append(softmax_output[:, 1])  # Collect the scores for the positive class\n",
    "        pred.append(softmax_output.max(dim=1)[1])\n",
    "        label.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_scores = torch.cat(scores, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    epoch_sen = tp / (tp + fn)\n",
    "    epoch_spe = tn / (tn + fp)\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "    epoch_f1 = f1_score(y_true, y_pred)\n",
    "    epoch_auc = roc_auc_score(y_true, y_scores)  # Use scores instead of predicted labels\n",
    "    return epoch_sen, epoch_spe, epoch_acc, epoch_f1, epoch_auc,loss_all / len(loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 200)\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_new =  np.load(f'{dataset_name}/{atlas_name}/X.npz')\n",
    "X_loaded = [X_new[key] for key in X_new.files]\n",
    "X_loaded = [normalize(matrix) for matrix in X_loaded]\n",
    "\n",
    "print(X_loaded[0].shape)\n",
    "\n",
    "Y_loaded = np.load(f'{dataset_name}/{atlas_name}/Y.npy')\n",
    "print(Y_loaded)\n",
    "\n",
    "# Load the CSV file\n",
    "demographic_data = pd.read_csv(f'{dataset_name}/{atlas_name}/demographics_data.csv')\n",
    "\n",
    "# Convert demographic data to tensor\n",
    "# Assuming the columns are ['subject_id', 'age', 'edu', 'motion', 'sex']\n",
    "demographics = demographic_data[['Age', 'Edu','Sex']].values\n",
    "\n",
    "# sites_labels = np.load(f'{dataset_name}/site_labels.npy')\n",
    "# print(sites_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n",
      "606\n",
      "964\n",
      "964\n",
      "(606, 3)\n",
      "(964, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store segregated data\n",
    "X_male, Y_male = [], []\n",
    "X_female, Y_female = [], []\n",
    "demographics_male, demographics_female = [], []\n",
    "\n",
    "# Segregate data based on sex\n",
    "for i, sex in enumerate(demographic_data['Sex']):\n",
    "    if sex == 0:  # Male\n",
    "        X_male.append(X_loaded[i])\n",
    "        Y_male.append(Y_loaded[i])\n",
    "        demographics_male.append(demographics[i])\n",
    "    elif sex == 1:  # Female\n",
    "        X_female.append(X_loaded[i])\n",
    "        Y_female.append(Y_loaded[i])\n",
    "        demographics_female.append(demographics[i])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "Y_male = np.array(Y_male)\n",
    "Y_female = np.array(Y_female)\n",
    "demographics_male = np.array(demographics_male)\n",
    "demographics_female = np.array(demographics_female)\n",
    "\n",
    "# Print the lengths of the segregated data\n",
    "print(len(X_male))\n",
    "print(len(Y_male))\n",
    "print(len(X_female))\n",
    "print(len(Y_female))\n",
    "print(demographics_male.shape)\n",
    "print(demographics_female.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "device = torch.device('cpu')\n",
    "eval_metrics1 = np.zeros((skf.n_splits, 5))\n",
    "eval_metrics2 = np.zeros((skf.n_splits, 5))\n",
    "eval_metrics3 = np.zeros((skf.n_splits, 5))\n",
    "eval_metrics4 = np.zeros((skf.n_splits, 5))\n",
    "\n",
    "dataset = X_loaded\n",
    "labels = Y_loaded\n",
    "# demographics = demographics_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_val_model = SkipConnModel((end - start + 4), 2, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset =  1570\n",
      "labels =  1570\n",
      "demogr =  1570\n",
      "CV: 001, Epoch: 001, Val Loss: 16.74726, Val ACC: 0.71831,Val AUC: 0.76948, Test ACC: 0.61783, Test F1: 0.68085, TEST SPE: 0.44000, TEST SEN: 0.78049, TEST AUC: 0.64862\n",
      "CV: 001, Epoch: 002, Val Loss: 16.11673, Val ACC: 0.73239,Val AUC: 0.79452, Test ACC: 0.63694, Test F1: 0.69841, TEST SPE: 0.45333, TEST SEN: 0.80488, TEST AUC: 0.67122\n",
      "CV: 001, Epoch: 003, Val Loss: 15.10120, Val ACC: 0.76056,Val AUC: 0.82492, Test ACC: 0.63057, Test F1: 0.68132, TEST SPE: 0.49333, TEST SEN: 0.75610, TEST AUC: 0.68520\n",
      "CV: 001, Epoch: 004, Val Loss: 16.05330, Val ACC: 0.69014,Val AUC: 0.80743, Test ACC: 0.63057, Test F1: 0.71000, TEST SPE: 0.37333, TEST SEN: 0.86585, TEST AUC: 0.68016\n",
      "CV: 001, Epoch: 005, Val Loss: 15.02713, Val ACC: 0.76761,Val AUC: 0.81558, Test ACC: 0.63694, Test F1: 0.66667, TEST SPE: 0.57333, TEST SEN: 0.69512, TEST AUC: 0.68260\n",
      "CV: 001, Epoch: 006, Val Loss: 16.56324, Val ACC: 0.66197,Val AUC: 0.81995, Test ACC: 0.58599, Test F1: 0.69484, TEST SPE: 0.24000, TEST SEN: 0.90244, TEST AUC: 0.68553\n",
      "CV: 001, Epoch: 007, Val Loss: 17.57781, Val ACC: 0.66901,Val AUC: 0.78140, Test ACC: 0.64968, Test F1: 0.60993, TEST SPE: 0.78667, TEST SEN: 0.52439, TEST AUC: 0.69187\n",
      "CV: 001, Epoch: 008, Val Loss: 33.55947, Val ACC: 0.61972,Val AUC: 0.75556, Test ACC: 0.53503, Test F1: 0.30476, TEST SPE: 0.90667, TEST SEN: 0.19512, TEST AUC: 0.61171\n",
      "CV: 001, Epoch: 009, Val Loss: 47.44418, Val ACC: 0.52817,Val AUC: 0.74881, Test ACC: 0.53503, Test F1: 0.69198, TEST SPE: 0.02667, TEST SEN: 1.00000, TEST AUC: 0.73837\n",
      "CV: 001, Epoch: 010, Val Loss: 15.08716, Val ACC: 0.73944,Val AUC: 0.82134, Test ACC: 0.61146, Test F1: 0.67027, TEST SPE: 0.45333, TEST SEN: 0.75610, TEST AUC: 0.68260\n",
      "CV: 001, Epoch: 011, Val Loss: 27.37324, Val ACC: 0.61972,Val AUC: 0.68760, Test ACC: 0.64331, Test F1: 0.71717, TEST SPE: 0.40000, TEST SEN: 0.86585, TEST AUC: 0.68114\n",
      "CV: 001, Epoch: 012, Val Loss: 39.53221, Val ACC: 0.61972,Val AUC: 0.68025, Test ACC: 0.62420, Test F1: 0.70647, TEST SPE: 0.36000, TEST SEN: 0.86585, TEST AUC: 0.65805\n",
      "CV: 001, Epoch: 013, Val Loss: 22.99189, Val ACC: 0.61972,Val AUC: 0.79511, Test ACC: 0.60510, Test F1: 0.50000, TEST SPE: 0.85333, TEST SEN: 0.37805, TEST AUC: 0.71431\n",
      "CV: 001, Epoch: 014, Val Loss: 35.20163, Val ACC: 0.61972,Val AUC: 0.72317, Test ACC: 0.61146, Test F1: 0.70531, TEST SPE: 0.30667, TEST SEN: 0.89024, TEST AUC: 0.67967\n",
      "CV: 001, Epoch: 015, Val Loss: 18.82113, Val ACC: 0.69718,Val AUC: 0.76649, Test ACC: 0.58599, Test F1: 0.65608, TEST SPE: 0.40000, TEST SEN: 0.75610, TEST AUC: 0.66146\n",
      "CV: 001, Epoch: 016, Val Loss: 40.45311, Val ACC: 0.57746,Val AUC: 0.75040, Test ACC: 0.52229, Test F1: 0.24242, TEST SPE: 0.93333, TEST SEN: 0.14634, TEST AUC: 0.64016\n",
      "CV: 001, Epoch: 017, Val Loss: 28.73714, Val ACC: 0.59859,Val AUC: 0.69058, Test ACC: 0.52866, Test F1: 0.56471, TEST SPE: 0.46667, TEST SEN: 0.58537, TEST AUC: 0.59089\n",
      "CV: 001, Epoch: 018, Val Loss: 43.51558, Val ACC: 0.51408,Val AUC: 0.77166, Test ACC: 0.53503, Test F1: 0.69198, TEST SPE: 0.02667, TEST SEN: 1.00000, TEST AUC: 0.66780\n",
      "CV: 001, Epoch: 019, Val Loss: 48.68010, Val ACC: 0.54225,Val AUC: 0.68462, Test ACC: 0.54140, Test F1: 0.69492, TEST SPE: 0.04000, TEST SEN: 1.00000, TEST AUC: 0.67268\n",
      "CV: 001, Epoch: 020, Val Loss: 35.47571, Val ACC: 0.57746,Val AUC: 0.67548, Test ACC: 0.54777, Test F1: 0.51701, TEST SPE: 0.64000, TEST SEN: 0.46341, TEST AUC: 0.64081\n",
      "CV: 001, Epoch: 021, Val Loss: 33.38692, Val ACC: 0.61268,Val AUC: 0.72635, Test ACC: 0.53503, Test F1: 0.29126, TEST SPE: 0.92000, TEST SEN: 0.18293, TEST AUC: 0.68488\n",
      "CV: 001, Epoch: 022, Val Loss: 23.89345, Val ACC: 0.65493,Val AUC: 0.73808, Test ACC: 0.60510, Test F1: 0.67021, TEST SPE: 0.42667, TEST SEN: 0.76829, TEST AUC: 0.64537\n",
      "CV: 001, Epoch: 023, Val Loss: 54.14212, Val ACC: 0.54225,Val AUC: 0.68164, Test ACC: 0.55414, Test F1: 0.70085, TEST SPE: 0.06667, TEST SEN: 1.00000, TEST AUC: 0.70211\n",
      "CV: 001, Epoch: 024, Val Loss: 84.14006, Val ACC: 0.51408,Val AUC: 0.80763, Test ACC: 0.47134, Test F1: 0.02353, TEST SPE: 0.97333, TEST SEN: 0.01220, TEST AUC: 0.68407\n",
      "CV: 001, Epoch: 025, Val Loss: 36.71908, Val ACC: 0.59155,Val AUC: 0.62540, Test ACC: 0.60510, Test F1: 0.66304, TEST SPE: 0.45333, TEST SEN: 0.74390, TEST AUC: 0.63415\n",
      "CV: 001, Epoch: 026, Val Loss: 22.30274, Val ACC: 0.69014,Val AUC: 0.75397, Test ACC: 0.63057, Test F1: 0.62821, TEST SPE: 0.66667, TEST SEN: 0.59756, TEST AUC: 0.65138\n",
      "CV: 001, Epoch: 027, Val Loss: 27.21964, Val ACC: 0.72535,Val AUC: 0.77126, Test ACC: 0.63057, Test F1: 0.61842, TEST SPE: 0.69333, TEST SEN: 0.57317, TEST AUC: 0.69837\n",
      "CV: 001, Epoch: 028, Val Loss: 63.22036, Val ACC: 0.57042,Val AUC: 0.69058, Test ACC: 0.49045, Test F1: 0.16667, TEST SPE: 0.92000, TEST SEN: 0.09756, TEST AUC: 0.64488\n",
      "CV: 001, Epoch: 029, Val Loss: 25.33200, Val ACC: 0.66197,Val AUC: 0.76014, Test ACC: 0.63057, Test F1: 0.68132, TEST SPE: 0.49333, TEST SEN: 0.75610, TEST AUC: 0.66650\n",
      "CV: 001, Epoch: 030, Val Loss: 38.14695, Val ACC: 0.55634,Val AUC: 0.66176, Test ACC: 0.63057, Test F1: 0.65882, TEST SPE: 0.57333, TEST SEN: 0.68293, TEST AUC: 0.66667\n",
      "CV: 001, Epoch: 031, Val Loss: 41.72387, Val ACC: 0.59859,Val AUC: 0.67051, Test ACC: 0.56688, Test F1: 0.54667, TEST SPE: 0.64000, TEST SEN: 0.50000, TEST AUC: 0.64163\n",
      "CV: 001, Epoch: 032, Val Loss: 55.67550, Val ACC: 0.54930,Val AUC: 0.62619, Test ACC: 0.50955, Test F1: 0.52174, TEST SPE: 0.50667, TEST SEN: 0.51220, TEST AUC: 0.52748\n",
      "CV: 001, Epoch: 033, Val Loss: 34.30507, Val ACC: 0.57042,Val AUC: 0.65302, Test ACC: 0.53503, Test F1: 0.56805, TEST SPE: 0.48000, TEST SEN: 0.58537, TEST AUC: 0.56504\n",
      "CV: 001, Epoch: 034, Val Loss: 26.21762, Val ACC: 0.67606,Val AUC: 0.76252, Test ACC: 0.62420, Test F1: 0.59310, TEST SPE: 0.73333, TEST SEN: 0.52439, TEST AUC: 0.69545\n",
      "CV: 001, Epoch: 035, Val Loss: 24.73028, Val ACC: 0.61972,Val AUC: 0.70509, Test ACC: 0.59873, Test F1: 0.65946, TEST SPE: 0.44000, TEST SEN: 0.74390, TEST AUC: 0.61252\n",
      "CV: 001, Epoch: 036, Val Loss: 24.70935, Val ACC: 0.63380,Val AUC: 0.74921, Test ACC: 0.59873, Test F1: 0.57718, TEST SPE: 0.68000, TEST SEN: 0.52439, TEST AUC: 0.67268\n",
      "CV: 001, Epoch: 037, Val Loss: 42.41426, Val ACC: 0.61268,Val AUC: 0.70330, Test ACC: 0.56051, Test F1: 0.67606, TEST SPE: 0.21333, TEST SEN: 0.87805, TEST AUC: 0.61902\n",
      "CV: 001, Epoch: 038, Val Loss: 39.07572, Val ACC: 0.61268,Val AUC: 0.74006, Test ACC: 0.53503, Test F1: 0.34234, TEST SPE: 0.86667, TEST SEN: 0.23171, TEST AUC: 0.67724\n",
      "CV: 001, Epoch: 039, Val Loss: 21.03830, Val ACC: 0.66901,Val AUC: 0.75556, Test ACC: 0.66242, Test F1: 0.69364, TEST SPE: 0.58667, TEST SEN: 0.73171, TEST AUC: 0.67805\n",
      "CV: 001, Epoch: 040, Val Loss: 27.86094, Val ACC: 0.68310,Val AUC: 0.73669, Test ACC: 0.57325, Test F1: 0.62570, TEST SPE: 0.45333, TEST SEN: 0.68293, TEST AUC: 0.66000\n",
      "CV: 001, Epoch: 041, Val Loss: 54.25293, Val ACC: 0.55634,Val AUC: 0.75020, Test ACC: 0.54777, Test F1: 0.69787, TEST SPE: 0.05333, TEST SEN: 1.00000, TEST AUC: 0.68211\n",
      "CV: 001, Epoch: 042, Val Loss: 49.12835, Val ACC: 0.57746,Val AUC: 0.71741, Test ACC: 0.61783, Test F1: 0.72727, TEST SPE: 0.22667, TEST SEN: 0.97561, TEST AUC: 0.67252\n",
      "CV: 001, Epoch: 043, Val Loss: 26.73641, Val ACC: 0.68310,Val AUC: 0.71741, Test ACC: 0.63694, Test F1: 0.68852, TEST SPE: 0.49333, TEST SEN: 0.76829, TEST AUC: 0.64959\n",
      "CV: 001, Epoch: 044, Val Loss: 80.32682, Val ACC: 0.50704,Val AUC: 0.67190, Test ACC: 0.54140, Test F1: 0.69492, TEST SPE: 0.04000, TEST SEN: 1.00000, TEST AUC: 0.69325\n",
      "CV: 001, Epoch: 045, Val Loss: 37.90651, Val ACC: 0.62676,Val AUC: 0.74424, Test ACC: 0.61783, Test F1: 0.72222, TEST SPE: 0.25333, TEST SEN: 0.95122, TEST AUC: 0.71919\n",
      "CV: 001, Epoch: 046, Val Loss: 45.17956, Val ACC: 0.52113,Val AUC: 0.71622, Test ACC: 0.59236, Test F1: 0.71429, TEST SPE: 0.17333, TEST SEN: 0.97561, TEST AUC: 0.69154\n",
      "CV: 001, Epoch: 047, Val Loss: 41.25918, Val ACC: 0.62676,Val AUC: 0.73390, Test ACC: 0.56051, Test F1: 0.47328, TEST SPE: 0.76000, TEST SEN: 0.37805, TEST AUC: 0.65382\n",
      "CV: 001, Epoch: 048, Val Loss: 45.21686, Val ACC: 0.61972,Val AUC: 0.71165, Test ACC: 0.59236, Test F1: 0.70909, TEST SPE: 0.20000, TEST SEN: 0.95122, TEST AUC: 0.66520\n",
      "CV: 001, Epoch: 049, Val Loss: 48.54972, Val ACC: 0.60563,Val AUC: 0.66832, Test ACC: 0.59236, Test F1: 0.67677, TEST SPE: 0.34667, TEST SEN: 0.81707, TEST AUC: 0.64984\n",
      "CV: 001, Epoch: 050, Val Loss: 72.34679, Val ACC: 0.57746,Val AUC: 0.75735, Test ACC: 0.50318, Test F1: 0.15217, TEST SPE: 0.96000, TEST SEN: 0.08537, TEST AUC: 0.65382\n"
     ]
    }
   ],
   "source": [
    "# Assuming skf is defined as an instance of StratifiedKFold\n",
    "for n_fold, (train_val, test) in enumerate(skf.split(dataset, labels)):\n",
    "    print('dataset = ',len(dataset))\n",
    "    print('labels = ',len(labels))\n",
    "    print('demogr = ',len(demographics))\n",
    "    model = SkipConnModel((end - start + 3), 2,3).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "    # Split dataset, labels, and demographics into train/validation and test sets\n",
    "    train_val_dataset = [dataset[i] for i in train_val]\n",
    "    test_dataset = [dataset[i] for i in test]\n",
    "    train_val_labels = labels[train_val]\n",
    "    test_labels = labels[test]\n",
    "    train_val_demographics = demographics[train_val]\n",
    "    test_demographics = demographics[test]\n",
    "\n",
    "    # Create index array for train_val_dataset\n",
    "    train_val_index = np.arange(len(train_val_dataset))\n",
    "\n",
    "    # Split train_val into train and validation sets\n",
    "    train_idx, val_idx, _, _ = train_test_split(\n",
    "        train_val_index,\n",
    "        train_val_labels,\n",
    "        test_size=0.1,\n",
    "        shuffle=True,\n",
    "        stratify=train_val_labels, \n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    train_dataset = [train_val_dataset[i] for i in train_idx]\n",
    "    val_dataset = [train_val_dataset[i] for i in val_idx]\n",
    "    train_labels = [train_val_labels[i] for i in train_idx]\n",
    "    val_labels = [train_val_labels[i] for i in val_idx]\n",
    "    train_demographics = [train_val_demographics[i] for i in train_idx]\n",
    "    val_demographics = [train_val_demographics[i] for i in val_idx]\n",
    "\n",
    "    X_train_featgraph, X_train_adjgraph, Y_train, _ = create_graph_sliding_window_demographics(train_dataset, train_demographics, train_labels, start, end, region=True)\n",
    "    X_val_featgraph, X_val_adjgraph, Y_val = create_graph_demographics(val_dataset, val_demographics, val_labels, start, end, region=True)\n",
    "    X_test_featgraph, X_test_adjgraph, Y_test = create_graph_demographics(test_dataset, test_demographics, test_labels, start, end, region=True)\n",
    "\n",
    "    X_train_datalist = to_tensor(X_train_featgraph, X_train_adjgraph, Y_train)\n",
    "    X_val_datalist = to_tensor(X_val_featgraph, X_val_adjgraph, Y_val)\n",
    "    X_test_datalist = to_tensor(X_test_featgraph, X_test_adjgraph, Y_test)\n",
    "    \n",
    "    if(len(X_train_datalist)%32==1):\n",
    "        batch = 31\n",
    "    else:\n",
    "        batch = 32\n",
    "        \n",
    "    train_loader = DataLoader(X_train_datalist, batch_size=batch, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    val_loader = DataLoader(X_val_datalist, batch_size=32, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    test_loader = DataLoader(X_test_datalist, batch_size=32, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    min_v_loss1 = np.inf\n",
    "    best_val_acc1 = 0\n",
    "    best_test_acc1 = 0\n",
    "    best_test_f11 = 0\n",
    "    best_test_sen1 = 0\n",
    "    best_test_spe1 = 0\n",
    "    best_test_auc1 = 0\n",
    "\n",
    "    best_test_acc2 = 0\n",
    "    best_test_f12 = 0\n",
    "    best_test_sen2 = 0\n",
    "    best_test_spe2 = 0\n",
    "    best_test_auc2 = 0\n",
    "\n",
    "    best_val_acc3 = 0\n",
    "    best_test_acc3 = 0\n",
    "    best_test_f13 = 0\n",
    "    best_test_sen3 = 0\n",
    "    best_test_spe3 = 0\n",
    "    best_test_auc3 = 0\n",
    "\n",
    "    best_val_auc4 = 0\n",
    "    best_test_acc4 = 0\n",
    "    best_test_f14 = 0\n",
    "    best_test_sen4 = 0\n",
    "    best_test_spe4 = 0\n",
    "    best_test_auc4 = 0\n",
    "    for epoch in range(50):\n",
    "        _, _, _, _, t_loss = GCN_train(train_loader)\n",
    "        val_sen, val_spe, val_acc,val_f1, val_auc, v_loss = GCN_test(val_loader)\n",
    "        test_sen, test_spe, test_acc,test_f1, test_auc, _ = GCN_test(test_loader)\n",
    "        # if epoch == 0:\n",
    "        #     print('first = ',t_loss)\n",
    "        # if epoch == 99:\n",
    "        #     print('last = ',t_loss)\n",
    "\n",
    "        \n",
    "        #if np.isnan(val_sen) or np.isnan(val_spe) or np.isnan(val_acc) or np.isnan(val_f1) or np.isnan(val_auc):\n",
    "        #    print(f\"NaN found in validation metrics at epoch {epoch}\")\n",
    "        #    break\n",
    "\n",
    "        #if np.isnan(test_sen) or np.isnan(test_spe) or np.isnan(test_acc) or np.isnan(test_f1) or np.isnan(test_auc):\n",
    "        #    print(f\"NaN found in test metrics at epoch {epoch}\")\n",
    "        #    break\n",
    "\n",
    "        if min_v_loss1 > v_loss:\n",
    "            min_v_loss1 = v_loss\n",
    "            best_val_acc1 = val_acc\n",
    "            best_test_f11 = test_f1\n",
    "            best_test_sen1, best_test_spe1, best_test_acc1, best_test_auc1 = test_sen, test_spe, test_acc, test_auc\n",
    "            # torch.save(model.state_dict(), 'SW_LANCET_results/model_min_vloss/best_model_%02i.pth' % (n_fold+1))\n",
    "        \n",
    "        if test_acc > best_test_acc2:  # Check if current test accuracy is better than the best recorded\n",
    "            best_test_acc2 = test_acc\n",
    "            best_test_f12 = test_f1\n",
    "            best_test_sen2, best_test_spe2,best_test_auc2 = test_sen, test_spe, test_auc\n",
    "            # torch.save(model.state_dict(), 'SW_LANCET_results/model_max_test_acc/best_model_%02i.pth' % (n_fold+1))\n",
    "        \n",
    "        if val_acc > best_val_acc3:\n",
    "            best_val_acc3 = val_acc\n",
    "            best_test_f13 = test_f1\n",
    "            best_test_sen3, best_test_spe3, best_test_acc3, best_test_auc3 = test_sen, test_spe, test_acc, test_auc\n",
    "            # max_val_model = model\n",
    "            # Create the directory if it does not exist\n",
    "            os.makedirs('Analysis/Male_Female_Bregions', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'Analysis/Male_Female_Bregions/model_female_2.pth')\n",
    "        \n",
    "        if val_auc > best_val_auc4:\n",
    "            best_val_auc4 = val_auc\n",
    "            best_test_f14 = test_f1\n",
    "            best_test_sen4, best_test_spe4, best_test_acc4, best_test_auc4 = test_sen, test_spe, test_acc, test_auc\n",
    "            \n",
    "        print('CV: {:03d}, Epoch: {:03d}, Val Loss: {:.5f}, Val ACC: {:.5f},Val AUC: {:.5f}, Test ACC: {:.5f}, Test F1: {:.5f}, TEST SPE: {:.5f}, '\n",
    "                  'TEST SEN: {:.5f}, TEST AUC: {:.5f}'.format(n_fold +1, epoch + 1, v_loss, val_acc, val_auc, test_acc,test_f1,\n",
    "                                        test_spe,test_sen, test_auc))\n",
    "    \n",
    "            \n",
    "    eval_metrics1[n_fold, 0] = best_test_sen1\n",
    "    eval_metrics1[n_fold, 1] = best_test_spe1\n",
    "    eval_metrics1[n_fold, 2] = best_test_acc1\n",
    "    eval_metrics1[n_fold, 3] = best_test_f11\n",
    "    eval_metrics1[n_fold, 4] = best_test_auc1\n",
    "\n",
    "    eval_metrics2[n_fold, 0] = best_test_sen2\n",
    "    eval_metrics2[n_fold, 1] = best_test_spe2\n",
    "    eval_metrics2[n_fold, 2] = best_test_acc2\n",
    "    eval_metrics2[n_fold, 3] = best_test_f12\n",
    "    eval_metrics2[n_fold, 4] = best_test_auc2\n",
    "\n",
    "    eval_metrics3[n_fold, 0] = best_test_sen3\n",
    "    eval_metrics3[n_fold, 1] = best_test_spe3\n",
    "    eval_metrics3[n_fold, 2] = best_test_acc3\n",
    "    eval_metrics3[n_fold, 3] = best_test_f13\n",
    "    eval_metrics3[n_fold, 4] = best_test_auc3\n",
    "        \n",
    "    eval_metrics4[n_fold, 0] = best_test_sen4\n",
    "    eval_metrics4[n_fold, 1] = best_test_spe4\n",
    "    eval_metrics4[n_fold, 2] = best_test_acc4\n",
    "    eval_metrics4[n_fold, 3] = best_test_f14\n",
    "    eval_metrics4[n_fold, 4] = best_test_auc4\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corresponding to minimum val loss\n",
      "              SEN       SPE       ACC        F1   AUC-ROC\n",
      "Fold_01  0.695122  0.573333  0.636943  0.666667  0.682602\n",
      "Fold_02  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_03  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_04  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_05  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_06  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_07  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_08  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_09  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_10  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Average Sensitivity: 0.0695±0.2085\n",
      "Average Specificity: 0.0573±0.1720\n",
      "Average Accuracy: 0.0637±0.1911\n",
      "Average F1: 0.0667±0.2000\n",
      "Average AUC-ROC: 0.0683±0.2048\n",
      "\n",
      "Corresponding to best test acc\n",
      "              SEN       SPE      ACC        F1   AUC-ROC\n",
      "Fold_01  0.731707  0.586667  0.66242  0.693642  0.678049\n",
      "Fold_02  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_03  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_04  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_05  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_06  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_07  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_08  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_09  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Fold_10  0.000000  0.000000  0.00000  0.000000  0.000000\n",
      "Average Sensitivity: 0.0732±0.2195\n",
      "Average Specificity: 0.0587±0.1760\n",
      "Average Accuracy: 0.0662±0.1987\n",
      "Average F1: 0.0694±0.2081\n",
      "Average AUC-ROC: 0.0678±0.2034\n",
      "\n",
      "Corresponding to maxm val acc\n",
      "              SEN       SPE       ACC        F1   AUC-ROC\n",
      "Fold_01  0.695122  0.573333  0.636943  0.666667  0.682602\n",
      "Fold_02  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_03  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_04  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_05  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_06  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_07  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_08  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_09  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_10  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Average Sensitivity: 0.0695±0.2085\n",
      "Average Specificity: 0.0573±0.1720\n",
      "Average Accuracy: 0.0637±0.1911\n",
      "Average F1: 0.0667±0.2000\n",
      "Average AUC-ROC: 0.0683±0.2048\n",
      "\n",
      "Corresponding to maxm val AUC\n",
      "              SEN       SPE       ACC        F1   AUC-ROC\n",
      "Fold_01  0.756098  0.493333  0.630573  0.681319  0.685203\n",
      "Fold_02  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_03  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_04  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_05  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_06  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_07  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_08  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_09  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Fold_10  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "Average Sensitivity: 0.0756±0.2268\n",
      "Average Specificity: 0.0493±0.1480\n",
      "Average Accuracy: 0.0631±0.1892\n",
      "Average F1: 0.0681±0.2044\n",
      "Average AUC-ROC: 0.0685±0.2056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Corresponding to minimum val loss\")\n",
    "eval_df1 = pd.DataFrame(eval_metrics1)\n",
    "eval_df1.columns = ['SEN', 'SPE', 'ACC','F1', 'AUC-ROC']\n",
    "eval_df1.index = ['Fold_%02i' % (i + 1) for i in range(skf.n_splits)]\n",
    "print(eval_df1)\n",
    "print('Average Sensitivity: %.4f±%.4f' % (eval_metrics1[:, 0].mean(), eval_metrics1[:, 0].std()))\n",
    "print('Average Specificity: %.4f±%.4f' % (eval_metrics1[:, 1].mean(), eval_metrics1[:, 1].std()))\n",
    "print('Average Accuracy: %.4f±%.4f' % (eval_metrics1[:, 2].mean(), eval_metrics1[:, 2].std()))\n",
    "print('Average F1: %.4f±%.4f' % (eval_metrics1[:, 3].mean(), eval_metrics1[:, 3].std()))\n",
    "print('Average AUC-ROC: %.4f±%.4f' % (eval_metrics1[:, 4].mean(), eval_metrics1[:, 4].std()))\n",
    "\n",
    "\n",
    "print(\"\\nCorresponding to best test acc\")\n",
    "eval_df2 = pd.DataFrame(eval_metrics2)\n",
    "eval_df2.columns = ['SEN', 'SPE', 'ACC','F1', 'AUC-ROC']\n",
    "eval_df2.index = ['Fold_%02i' % (i + 1) for i in range(skf.n_splits)]\n",
    "print(eval_df2)\n",
    "print('Average Sensitivity: %.4f±%.4f' % (eval_metrics2[:, 0].mean(), eval_metrics2[:, 0].std()))\n",
    "print('Average Specificity: %.4f±%.4f' % (eval_metrics2[:, 1].mean(), eval_metrics2[:, 1].std()))\n",
    "print('Average Accuracy: %.4f±%.4f' % (eval_metrics2[:, 2].mean(), eval_metrics2[:, 2].std()))\n",
    "print('Average F1: %.4f±%.4f' % (eval_metrics2[:, 3].mean(), eval_metrics2[:, 3].std()))\n",
    "print('Average AUC-ROC: %.4f±%.4f' % (eval_metrics2[:, 4].mean(), eval_metrics2[:, 4].std()))\n",
    "\n",
    "print(\"\\nCorresponding to maxm val acc\")\n",
    "eval_df3 = pd.DataFrame(eval_metrics3)\n",
    "eval_df3.columns = ['SEN', 'SPE', 'ACC','F1', 'AUC-ROC']\n",
    "eval_df3.index = ['Fold_%02i' % (i + 1) for i in range(skf.n_splits)]\n",
    "print(eval_df3)\n",
    "print('Average Sensitivity: %.4f±%.4f' % (eval_metrics3[:, 0].mean(), eval_metrics3[:, 0].std()))\n",
    "print('Average Specificity: %.4f±%.4f' % (eval_metrics3[:, 1].mean(), eval_metrics3[:, 1].std()))\n",
    "print('Average Accuracy: %.4f±%.4f' % (eval_metrics3[:, 2].mean(), eval_metrics3[:, 2].std()))\n",
    "print('Average F1: %.4f±%.4f' % (eval_metrics3[:, 3].mean(), eval_metrics3[:, 3].std()))\n",
    "print('Average AUC-ROC: %.4f±%.4f' % (eval_metrics3[:, 4].mean(), eval_metrics3[:, 4].std()))\n",
    "\n",
    "print(\"\\nCorresponding to maxm val AUC\")\n",
    "eval_df4 = pd.DataFrame(eval_metrics4)\n",
    "eval_df4.columns = ['SEN', 'SPE', 'ACC','F1', 'AUC-ROC']\n",
    "eval_df4.index = ['Fold_%02i' % (i + 1) for i in range(skf.n_splits)]\n",
    "print(eval_df4)\n",
    "print('Average Sensitivity: %.4f±%.4f' % (eval_metrics4[:, 0].mean(), eval_metrics4[:, 0].std()))\n",
    "print('Average Specificity: %.4f±%.4f' % (eval_metrics4[:, 1].mean(), eval_metrics4[:, 1].std()))\n",
    "print('Average Accuracy: %.4f±%.4f' % (eval_metrics4[:, 2].mean(), eval_metrics4[:, 2].std()))\n",
    "print('Average F1: %.4f±%.4f' % (eval_metrics4[:, 3].mean(), eval_metrics4[:, 3].std()))\n",
    "print('Average AUC-ROC: %.4f±%.4f' % (eval_metrics4[:, 4].mean(), eval_metrics4[:, 4].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_cam(model, data):\n",
    "    data = data.to(device)\n",
    "    model.zero_grad()\n",
    "    data.x.requires_grad = True  # Set requires_grad to True for computing gradients\n",
    "\n",
    "    # Forward pass\n",
    "    output, _ = model(data)  # Get the prediction scores\n",
    "\n",
    "    # Apply softmax to calculate scores (probabilities)\n",
    "    scores = F.softmax(output, dim=1)\n",
    "    print(scores.shape)\n",
    "    print(scores[0])\n",
    "\n",
    "    # Extract MDD scores\n",
    "    scores_mdd = scores[:, 1]  # Assuming the second column corresponds to MDD\n",
    "    print('scores = ',scores_mdd.shape)\n",
    "\n",
    "    # Backward pass to compute gradients with respect to data.x\n",
    "    scores_mdd.backward(torch.ones_like(scores_mdd))  # Calculate gradients\n",
    "\n",
    "    # Retrieve the gradients of scores_mdd with respect to data.x\n",
    "    gradients = data.x.grad\n",
    "    print('gradients = ',gradients.shape)  # Check the shape of gradients\n",
    "\n",
    "    # Calculate Grad-CAM values by element-wise multiplying gradients and data.x\n",
    "    grad_cam = gradients * data.x\n",
    "    print('data.x = ',data.x.shape)  # Check the shape of Grad-CAM values\n",
    "    print('gradcam = ',grad_cam.shape)  # Check the shape of Grad-CAM values\n",
    "\n",
    "    # Sum the Grad-CAM values for each brain region (column-wise sum)\n",
    "    # contribution = grad_cam.sum(dim=0)\n",
    "    contribution = torch.mean(grad_cam, dim=1)\n",
    "    \n",
    "    print('contribution = ',contribution.shape)  # Check the shape of Grad-CAM values\n",
    "\n",
    "    return scores, grad_cam, contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "Data(x=[116, 119], edge_index=[2, 1336], edge_attr=[1336], y=[1])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7831, 0.2169], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3336, 0.6664], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2313, 0.7687], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.9044, 0.0956], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2244, 0.7756], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1455, 0.8545], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0141, 0.9859], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5235, 0.4765], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2718, 0.7282], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4003, 0.5997], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2966, 0.7034], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6047, 0.3953], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6421, 0.3579], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8479, 0.1521], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3787, 0.6213], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3672, 0.6328], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7931, 0.2069], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3170, 0.6830], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4642, 0.5358], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2352, 0.7648], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2137, 0.7863], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4658, 0.5342], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0578, 0.9422], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4882, 0.5118], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6559, 0.3441], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1611, 0.8389], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5037, 0.4963], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4053, 0.5947], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1198, 0.8802], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8312, 0.1688], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7520, 0.2480], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8294, 0.1706], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2165, 0.7835], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8399, 0.1601], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4204, 0.5796], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8381, 0.1619], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1887, 0.8113], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0015, 0.9985], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3682, 0.6318], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7469, 0.2531], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6260, 0.3740], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1359, 0.8641], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7799, 0.2201], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.9174, 0.0826], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8729, 0.1271], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7909, 0.2091], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7536, 0.2464], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8119, 0.1881], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6102, 0.3898], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8054, 0.1946], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7442, 0.2558], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0291, 0.9709], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5739, 0.4261], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2884, 0.7116], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8404, 0.1596], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3412, 0.6588], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4780, 0.5220], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8445, 0.1555], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5604, 0.4396], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6467, 0.3533], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1843, 0.8157], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2459, 0.7541], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6225, 0.3775], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5984, 0.4016], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0167, 0.9833], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6847, 0.3153], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7659, 0.2341], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3060, 0.6940], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3465, 0.6535], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5358, 0.4642], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2170, 0.7830], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2770, 0.7230], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8358, 0.1642], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7524, 0.2476], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4251, 0.5749], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8300, 0.1700], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1332, 0.8668], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3164, 0.6836], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0246, 0.9754], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2656, 0.7344], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2102, 0.7898], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6311, 0.3689], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6914, 0.3086], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4612, 0.5388], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6802, 0.3198], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5353, 0.4647], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1814, 0.8186], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7508, 0.2492], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5079, 0.4921], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5212, 0.4788], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0108, 0.9892], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5221, 0.4779], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5839, 0.4161], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8496, 0.1504], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7235, 0.2765], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8144, 0.1856], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0479, 0.9521], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.0556, 0.9444], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3472, 0.6528], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5704, 0.4296], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5098, 0.4902], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5012, 0.4988], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7356, 0.2644], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7904, 0.2096], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7836, 0.2164], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8075, 0.1925], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6520, 0.3480], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3568, 0.6432], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8435, 0.1565], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1364, 0.8636], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1470, 0.8530], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5242, 0.4758], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5878, 0.4122], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4604, 0.5396], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5808, 0.4192], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8149, 0.1851], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6400, 0.3600], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8917, 0.1083], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5543, 0.4457], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7729, 0.2271], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5065, 0.4935], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4134, 0.5866], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5222, 0.4778], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4666, 0.5334], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8167, 0.1833], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6053, 0.3947], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4236, 0.5764], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3309, 0.6691], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4973, 0.5027], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5775, 0.4225], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2839, 0.7161], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.8394, 0.1606], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2611, 0.7389], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.4775, 0.5225], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.2655, 0.7345], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7346, 0.2654], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.6671, 0.3329], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3438, 0.6562], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.5835, 0.4165], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.7531, 0.2469], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.1844, 0.8156], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "torch.Size([1, 2])\n",
      "tensor([0.3838, 0.6162], grad_fn=<SelectBackward0>)\n",
      "scores =  torch.Size([1])\n",
      "gradients =  torch.Size([116, 119])\n",
      "data.x =  torch.Size([116, 119])\n",
      "gradcam =  torch.Size([116, 119])\n",
      "contribution =  torch.Size([116])\n",
      "contri array =  (142, 116)\n",
      "contri mean =  (116,)\n",
      "Mean contribution shape: (116,)\n"
     ]
    }
   ],
   "source": [
    "val_loader2 = DataLoader(X_val_datalist, batch_size=1, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "test_loader2 = DataLoader(X_test_datalist, batch_size=1, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "print(len(X_val_datalist))\n",
    "print(X_test_datalist[0])\n",
    "\n",
    "best_test_model = SkipConnModel((end - start + 3), 2, 3).to(device)\n",
    "state_dict = torch.load('Analysis/Male_Female_Bregions/model_female_2.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "best_test_model.load_state_dict(state_dict)\n",
    "\n",
    "#Evaluate data\n",
    "best_test_model.eval()\n",
    "# Use the function\n",
    "# Initialize an empty list to store contributions from each batch\n",
    "contributions_list = []\n",
    "\n",
    "# Assuming val_loader2 is your validation data loader\n",
    "for batch in val_loader2:\n",
    "    scores, grad_cam, contribution = calculate_grad_cam(best_test_model, batch)\n",
    "    \n",
    "    # Move contribution tensor to CPU before appending to list\n",
    "    contribution = contribution.cpu()   \n",
    "    \n",
    "    # Append contribution to the list\n",
    "    contributions_list.append(contribution)\n",
    "\n",
    "# Convert contributions_list to a numpy array after moving all tensors to CPU\n",
    "contributions_array = np.array([item.detach().numpy() for item in contributions_list])\n",
    "print('contri array = ', contributions_array.shape)\n",
    "\n",
    "# Calculate the mean across all contributions\n",
    "mean_contribution = np.mean(contributions_array, axis=0)\n",
    "print('contri mean = ', mean_contribution.shape)\n",
    "\n",
    "# Print or use the mean_contribution as needed\n",
    "print(\"Mean contribution shape:\", mean_contribution.shape)\n",
    "# print(\"Mean contribution:\", mean_contribution)   \n",
    "    \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.63808028e-04, -2.27638084e-04,  4.52868517e-05, -9.02492029e-05,\n",
       "       -1.02553087e-04, -3.84232466e-04, -1.32203655e-04, -1.37665513e-04,\n",
       "       -2.96640268e-04, -2.93525140e-04, -2.33579878e-04, -1.95891698e-04,\n",
       "       -2.37408691e-04, -1.70582876e-04, -2.19712965e-04, -1.79993251e-04,\n",
       "       -1.74765359e-04,  5.17941189e-05, -6.00122366e-05,  3.83020961e-05,\n",
       "       -1.26342347e-04, -1.25104460e-04, -1.09362125e-04, -8.19016786e-05,\n",
       "        2.36086446e-04,  1.88090286e-04,  5.48485514e-06, -3.49083675e-05,\n",
       "       -1.47977014e-04, -1.29998065e-04,  1.22389843e-04, -6.57819764e-05,\n",
       "        1.16068150e-04,  4.18350055e-06,  1.75450737e-06,  1.09010643e-05,\n",
       "       -3.43017746e-05, -8.94023651e-06, -3.81912359e-05,  2.27526198e-05,\n",
       "       -3.13020391e-05,  3.41469568e-05, -1.50485284e-04, -1.61818243e-04,\n",
       "       -1.88093996e-04, -1.86147954e-04, -6.90087327e-05, -1.19757169e-04,\n",
       "       -1.72357250e-04, -1.69277875e-04, -7.95489177e-05,  3.46225679e-05,\n",
       "       -7.62178897e-05, -8.15918247e-05,  7.29064559e-05, -3.10098585e-05,\n",
       "       -4.38276729e-05, -1.15572213e-04,  4.85897726e-05, -1.07155342e-04,\n",
       "       -1.55144691e-04, -2.39611225e-04,  7.67466481e-05, -8.73986719e-05,\n",
       "       -6.04049783e-05, -1.99088623e-04,  3.09880415e-04,  1.26806510e-04,\n",
       "        6.21381769e-05,  2.72049674e-05, -2.79607833e-04, -2.09635982e-04,\n",
       "       -1.17059782e-04, -1.46169055e-04, -1.09396191e-04, -6.52416857e-05,\n",
       "       -1.26524013e-04, -1.21222402e-04, -1.64724206e-05,  8.08164259e-05,\n",
       "       -9.16359932e-05, -1.17010895e-04, -1.21838086e-04, -2.40440553e-04,\n",
       "       -1.64909085e-04, -5.95647471e-05, -1.94284905e-04, -4.93133812e-05,\n",
       "       -9.75428338e-05, -1.52103938e-04,  2.33113115e-05, -1.16322175e-04,\n",
       "       -1.54346533e-04, -1.44232792e-04, -9.99788972e-05, -4.04628663e-05,\n",
       "       -6.17422702e-05, -3.17505655e-05, -9.66920197e-05, -1.91489307e-05,\n",
       "       -1.97412664e-04, -4.98636000e-05, -2.63310358e-04, -1.30013796e-04,\n",
       "       -1.60365569e-04, -9.41568142e-05, -5.05215939e-05,  4.11653746e-05,\n",
       "       -6.82826139e-05, -1.48113613e-04,  1.39197087e-04, -1.19521712e-04,\n",
       "        7.79944021e-05, -6.89037624e-05, -1.55566988e-04, -1.09467685e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of sorted Grad-CAM values: [ 66  24  25 110  67  30  32  79 112  62  54  68  17  58   2 107  19  51\n",
      "  41  69  90  39  35  26  33  34  37  78  99  55  40  97  36  27  38  95\n",
      "  56  87 101 106  85  18  64  96  75  31 108 113  46  52  50  53  23  63\n",
      "   3  80 105  98  88  94   4  59  22  74 115  57  91  81  72 111  47  77\n",
      "  82  21  20  76  29 103   6   7  93  73  28 109  42  89  92  60 114 104\n",
      "  43   0  84  49  13  48  16  15  45  44  86  11 100  65  71  14   1  10\n",
      "  12  61  83 102  70   9   8   5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Assuming 'avg_grad_cam' is your tensor of average Grad-CAM values\n",
    "# Sort mean_contribution and get sorted indices\n",
    "sorted_indices = np.argsort(-mean_contribution)\n",
    "\n",
    "# print(\"Sorted Grad-CAM values:\", sorted_grad_cam_values)\n",
    "print(\"Indices of sorted Grad-CAM values:\", sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
