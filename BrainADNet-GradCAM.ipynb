{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.nn import GATConv, ChebConv\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import os.path as osp\n",
    "import csv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=89\n",
    "atlas_name= \"AAL\"\n",
    "dataset_name = \"MDDvHC\"\n",
    "\n",
    "\n",
    "if atlas_name == \"AAL\":\n",
    "    start = 0\n",
    "    end = 116\n",
    "elif atlas_name == \"Craddock\":\n",
    "    start = 228\n",
    "    end = 428\n",
    "elif atlas_name == \"Dosenbach\":\n",
    "    start = 1408\n",
    "    end = 1568\n",
    "else:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Environment variables for reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Set your seed\n",
    "set_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize(matrix):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_matrix = scaler.fit_transform(matrix)\n",
    "    return normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "\n",
    "def fisher_z_transform(correlation_matrix, epsilon=1e-5):\n",
    "    return 0.5 * np.log((1 + correlation_matrix) / (1 - correlation_matrix + epsilon))\n",
    "\n",
    "def to_tensor(X_featgraph, X_adjgraph, Y):\n",
    "    datalist = []\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        ty = Y[i]\n",
    "\n",
    "        y = torch.tensor([ty]).long()\n",
    "\n",
    "        adjacency = X_adjgraph[i]\n",
    "        feature = X_featgraph[i]\n",
    "\n",
    "        x = torch.from_numpy(feature).float()\n",
    "        adj= adjacency\n",
    "        adj = torch.from_numpy(adj).float()\n",
    "        edge_index, edge_attr = dense_to_sparse(adj)\n",
    "        \n",
    "        datalist.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y))\n",
    "\n",
    "    return datalist\n",
    "\n",
    "\n",
    "def compute_KNN_graph(matrix, k_degree=10):\n",
    "    \"\"\" Calculate the adjacency matrix from the connectivity matrix.\"\"\"\n",
    "\n",
    "    matrix = np.abs(matrix)\n",
    "    idx = np.argsort(-matrix)[:, 0:k_degree]\n",
    "    matrix.sort()\n",
    "    matrix = matrix[:, ::-1]\n",
    "    matrix = matrix[:, 0:k_degree]\n",
    "\n",
    "    A = adjacency(matrix, idx).astype(np.float32)\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def adjacency(dist, idx):\n",
    "\n",
    "    m, k = dist.shape\n",
    "    assert m, k == idx.shape\n",
    "    assert dist.min() >= 0\n",
    "\n",
    "    # Weight matrix.\n",
    "    I = np.arange(0, m).repeat(k)\n",
    "    J = idx.reshape(m * k)\n",
    "    V = dist.reshape(m * k)\n",
    "    W = coo_matrix((V, (I, J)), shape=(m, m))\n",
    "\n",
    "    # No self-connections.\n",
    "    W.setdiag(0)\n",
    "\n",
    "    # Non-directed graph.\n",
    "    bigger = W.T > W\n",
    "    W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
    "\n",
    "    return W.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sliding window\n",
    "def create_graph_sliding_window_demographics(X, D, Y, start, end, region=True):\n",
    "    S = 30 # Sliding Step\n",
    "    T = 60 # Window Size\n",
    "\n",
    "    X_adjgraph=[]\n",
    "    X_featgraph = []\n",
    "    Y_list = []\n",
    "    num_samples_per_subject = []\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        bold_matrix = X[i]\n",
    "        n = bold_matrix.shape[0]\n",
    "        demog = D[i]\n",
    "        demog_expanded = np.expand_dims(demog, axis=0) \n",
    "        demog_expanded = np.repeat(demog_expanded, n, axis=0)\n",
    "        \n",
    "        temp_y = Y[i]\n",
    "\n",
    "        num_rows, num_cols = bold_matrix.shape\n",
    "        num_samples = 0\n",
    "        for start_idx in range(0, num_cols - T + 1, S):\n",
    "            end_idx = start_idx + T\n",
    "            if end_idx <= num_cols:\n",
    "                \n",
    "                if region == True:\n",
    "                    window_data =  bold_matrix[:, start_idx:end_idx]    #RxR\n",
    "                else:\n",
    "                    window_data =  np.transpose(bold_matrix[:, start_idx:end_idx])  #TxT\n",
    "\n",
    "                window_data1 = np.corrcoef(window_data)\n",
    "                correlation_matrix_fisher = fisher_z_transform(window_data1)\n",
    "                correlation_matrix_fisher = np.around(correlation_matrix_fisher, 8)     #upto 8 decimal points\n",
    "\n",
    "                result_matrix = np.concatenate((correlation_matrix_fisher, demog_expanded), axis=1)\n",
    "\n",
    "                \n",
    "                knn_graph = compute_KNN_graph(correlation_matrix_fisher)\n",
    "\n",
    "                if region == True:\n",
    "                    X_featgraph.append(result_matrix)\n",
    "                else:\n",
    "                    X_featgraph.append(window_data)\n",
    "                    \n",
    "                X_adjgraph.append(knn_graph)\n",
    "                Y_list.append(temp_y)\n",
    "                num_samples = num_samples+1\n",
    "            \n",
    "        num_samples_per_subject.append(num_samples)\n",
    "\n",
    "\n",
    "    return X_featgraph, X_adjgraph, Y_list, num_samples_per_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_demographics(X, D, Y, start, end, region=True):\n",
    "    X_adjgraph=[]\n",
    "    X_featgraph = []\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        if region == True:\n",
    "            bold_matrix = X[i]  #RxR\n",
    "        else:\n",
    "            bold_matrix = np.transpose(X[i]) #TxT\n",
    "\n",
    "        n= bold_matrix.shape[0]\n",
    "       \n",
    "        demog_expanded = np.expand_dims(D[i], axis=0)\n",
    "        demog_expanded = np.repeat(demog_expanded, n, axis=0) \n",
    "        \n",
    "        window_data1 = np.corrcoef(bold_matrix)\n",
    "        correlation_matrix_fisher = fisher_z_transform(window_data1)\n",
    "        correlation_matrix_fisher = np.around(correlation_matrix_fisher, 8)\n",
    "        knn_graph = compute_KNN_graph(correlation_matrix_fisher)\n",
    "        result_matrix = np.concatenate((correlation_matrix_fisher, demog_expanded), axis=1)\n",
    "\n",
    "        if region == True:\n",
    "            X_featgraph.append(result_matrix)\n",
    "        else:\n",
    "            X_featgraph.append(bold_matrix)\n",
    "            \n",
    "        X_adjgraph.append(knn_graph)\n",
    "\n",
    "    return X_featgraph, X_adjgraph, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv, global_mean_pool,GATConv\n",
    "\n",
    "\n",
    "\n",
    "class SkipConnModel(nn.Module):\n",
    "    def __init__(self, num_features_R, num_classes, k_order, dropout_prob=0.5):\n",
    "        super(SkipConnModel, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        self.num_layers = 6\n",
    "        self.convs = nn.ModuleList() \n",
    "        self.bns = nn.ModuleList()   \n",
    "        \n",
    "        self.convs.append(ChebConv(num_features_R, 128, K=3, normalization='sym'))\n",
    "        self.bns.append(nn.BatchNorm1d(128))\n",
    "        \n",
    "        self.convs.append(ChebConv(128, 128, K=3, normalization='sym'))\n",
    "        self.bns.append(nn.BatchNorm1d(128))\n",
    "\n",
    "        self.convs.append(ChebConv(128, 128, K=3, normalization='sym'))\n",
    "        self.bns.append(nn.BatchNorm1d(128))\n",
    "        \n",
    "        \n",
    "        self.out_fc = nn.Linear(128, num_classes)\n",
    "        self.weights = torch.nn.Parameter(torch.randn(len(self.convs)))\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        self.out_fc.reset_parameters()\n",
    "        torch.nn.init.normal_(self.weights)\n",
    "\n",
    "    def forward(self, data_R):\n",
    "        x1, edge_index1, edge_attr1 = data_R.x, data_R.edge_index, data_R.edge_attr\n",
    "        batch1 = data_R.batch\n",
    "        \n",
    "        layer_out1 = []  \n",
    "        x1 = self.convs[0](x1, edge_index1, edge_attr1)\n",
    "        x1 = self.bns[0](x1)\n",
    "        x1 = F.relu(x1, inplace=True)\n",
    "        layer_out1.append(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout_prob, training=self.training)\n",
    "        \n",
    "                          \n",
    "        x1 = self.convs[1](x1, edge_index1, edge_attr1)\n",
    "        x1 = self.bns[1](x1)\n",
    "        x1 = F.relu(x1, inplace=True)\n",
    "        x1 = x1 + 0.8 * layer_out1[0]\n",
    "        layer_out1.append(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout_prob, training=self.training) \n",
    "        \n",
    "                          \n",
    "        x1 = self.convs[2](x1, edge_index1, edge_attr1)\n",
    "        x1 = self.bns[2](x1)\n",
    "        x1 = F.relu(x1, inplace=True)\n",
    "        x1 = x1 + 0.8 * layer_out1[1]\n",
    "        layer_out1.append(x1)\n",
    "       \n",
    "        weight = F.softmax(self.weights, dim=0)\n",
    "        weighted_outs = [layer_out1[i] * weight[i] for i in range(len(layer_out1))]\n",
    "        emb = sum(weighted_outs)\n",
    "        pooled_emb = global_mean_pool(emb, batch1)\n",
    "        x = self.out_fc(pooled_emb)\n",
    "        \n",
    "        return x, pooled_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DECOV(embeddings):\n",
    "    embeddings_t = embeddings.T\n",
    "    C = torch.cov(embeddings_t)\n",
    "    C_fro_norm = torch.norm(C, p='fro')\n",
    "    diag_elements = torch.diag(C,0)\n",
    "    C2_l2norm_diag = torch.norm(diag_elements)\n",
    "    L_DECOV = (C_fro_norm ** 2) - (C2_l2norm_diag ** 2)\n",
    "\n",
    "    return L_DECOV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCN_train(loader):\n",
    "    model.train()\n",
    "\n",
    "    pred = []\n",
    "    label = []\n",
    "    loss_all = 0\n",
    "    alpha= 1\n",
    "    beta = 1e-8\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, pooled = model(data)\n",
    "\n",
    "        pooled = pooled.to('cpu')\n",
    "        loss_decov = DECOV(pooled)\n",
    "        loss_decov = loss_decov.to(device)\n",
    "        pooled = pooled.to(device)\n",
    "\n",
    "        loss_ce = func.cross_entropy(output, data.y)\n",
    "        loss = alpha * loss_ce + beta *loss_decov\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred.append(func.softmax(output, dim=1).max(dim=1)[1])\n",
    "        label.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    epoch_sen = tp / (tp + fn)\n",
    "    epoch_spe = tn / (tn + fp)\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return epoch_sen, epoch_spe, epoch_acc, f1, loss_all/len(loader)\n",
    "\n",
    "\n",
    "def GCN_test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    scores = []\n",
    "    label = []\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output , pooled= model(data)\n",
    "\n",
    "        loss_ce = func.cross_entropy(output, data.y)\n",
    "        loss = loss_ce\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "\n",
    "        softmax_output = func.softmax(output, dim=1)\n",
    "        scores.append(softmax_output[:, 1])\n",
    "        pred.append(softmax_output.max(dim=1)[1])\n",
    "        label.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_scores = torch.cat(scores, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    epoch_sen = tp / (tp + fn)\n",
    "    epoch_spe = tn / (tn + fp)\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "    epoch_f1 = f1_score(y_true, y_pred)\n",
    "    epoch_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    return epoch_sen, epoch_spe, epoch_acc, epoch_f1, epoch_auc,loss_all / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_new =  np.load(f'./{dataset_name}/{atlas_name}/X.npz')\n",
    "X_loaded = [X_new[key] for key in X_new.files]\n",
    "X_loaded = [normalize(matrix) for matrix in X_loaded]\n",
    "print(X_loaded[0].shape)\n",
    "Y_loaded = np.load(f'./{dataset_name}/{atlas_name}/Y.npy')\n",
    "print(Y_loaded)\n",
    "demographic_data = pd.read_csv(f'./{dataset_name}/{atlas_name}/demographics_data.csv')\n",
    "demographics = demographic_data[['Age', 'Edu', 'Sex']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_male, Y_male = [], []\n",
    "X_female, Y_female = [], []\n",
    "demographics_male, demographics_female = [], []\n",
    "\n",
    "for i, sex in enumerate(demographic_data['Sex']):\n",
    "    if sex == 0:  \n",
    "        X_male.append(X_loaded[i])\n",
    "        Y_male.append(Y_loaded[i])\n",
    "        demographics_male.append(demographics[i])\n",
    "    elif sex == 1:  \n",
    "        X_female.append(X_loaded[i])\n",
    "        Y_female.append(Y_loaded[i])\n",
    "        demographics_female.append(demographics[i])\n",
    "\n",
    "Y_male = np.array(Y_male)\n",
    "Y_female = np.array(Y_female)\n",
    "demographics_male = np.array(demographics_male)\n",
    "demographics_female = np.array(demographics_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "device = torch.device('cpu')\n",
    "eval_metrics2 = np.zeros((skf.n_splits, 5))\n",
    "eval_metrics3 = np.zeros((skf.n_splits, 5))\n",
    "\n",
    "dataset = X_loaded\n",
    "labels = Y_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_fold, (train_val, test) in enumerate(skf.split(dataset, labels)):\n",
    "\n",
    "    model = SkipConnModel((end - start + 3), 2,3).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "    train_val_dataset = [dataset[i] for i in train_val]\n",
    "    test_dataset = [dataset[i] for i in test]\n",
    "    train_val_labels = labels[train_val]\n",
    "    test_labels = labels[test]\n",
    "    train_val_demographics = demographics[train_val]\n",
    "    test_demographics = demographics[test]\n",
    "\n",
    "    train_val_index = np.arange(len(train_val_dataset))\n",
    "    train_idx, val_idx, _, _ = train_test_split(\n",
    "        train_val_index,\n",
    "        train_val_labels,\n",
    "        test_size=0.1,\n",
    "        shuffle=True,\n",
    "        stratify=train_val_labels, \n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    train_dataset = [train_val_dataset[i] for i in train_idx]\n",
    "    val_dataset = [train_val_dataset[i] for i in val_idx]\n",
    "    train_labels = [train_val_labels[i] for i in train_idx]\n",
    "    val_labels = [train_val_labels[i] for i in val_idx]\n",
    "    train_demographics = [train_val_demographics[i] for i in train_idx]\n",
    "    val_demographics = [train_val_demographics[i] for i in val_idx]\n",
    "\n",
    "    X_train_featgraph, X_train_adjgraph, Y_train, _ = create_graph_sliding_window_demographics(train_dataset, train_demographics, train_labels, start, end, region=True)\n",
    "    X_val_featgraph, X_val_adjgraph, Y_val = create_graph_demographics(val_dataset, val_demographics, val_labels, start, end, region=True)\n",
    "    X_test_featgraph, X_test_adjgraph, Y_test = create_graph_demographics(test_dataset, test_demographics, test_labels, start, end, region=True)\n",
    "\n",
    "    X_train_datalist = to_tensor(X_train_featgraph, X_train_adjgraph, Y_train)\n",
    "    X_val_datalist = to_tensor(X_val_featgraph, X_val_adjgraph, Y_val)\n",
    "    X_test_datalist = to_tensor(X_test_featgraph, X_test_adjgraph, Y_test)\n",
    "    \n",
    "           \n",
    "    train_loader = DataLoader(X_train_datalist, batch_size=32, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    val_loader = DataLoader(X_val_datalist, batch_size=32, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "    test_loader = DataLoader(X_test_datalist, batch_size=32, shuffle=True, num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    best_test_acc2 = 0\n",
    "    best_test_f12 = 0\n",
    "    best_test_sen2 = 0\n",
    "    best_test_spe2 = 0\n",
    "    best_test_auc2 = 0\n",
    "\n",
    "    best_val_acc3 = 0\n",
    "    best_test_acc3 = 0\n",
    "    best_test_f13 = 0\n",
    "    best_test_sen3 = 0\n",
    "    best_test_spe3 = 0\n",
    "    best_test_auc3 = 0\n",
    "\n",
    "    for epoch in range(50):\n",
    "        _, _, _, _, t_loss = GCN_train(train_loader)\n",
    "        val_sen, val_spe, val_acc,val_f1, val_auc, v_loss = GCN_test(val_loader)\n",
    "        test_sen, test_spe, test_acc,test_f1, test_auc, _ = GCN_test(test_loader)\n",
    "        \n",
    "        \n",
    "        if test_acc > best_test_acc2: \n",
    "            best_test_acc2 = test_acc\n",
    "            best_test_f12 = test_f1\n",
    "            best_test_sen2, best_test_spe2,best_test_auc2 = test_sen, test_spe, test_auc\n",
    "        \n",
    "        if val_acc > best_val_acc3:\n",
    "            best_val_acc3 = val_acc\n",
    "            best_test_f13 = test_f1\n",
    "            best_test_sen3, best_test_spe3, best_test_acc3, best_test_auc3 = test_sen, test_spe, test_acc, test_auc\n",
    "            \n",
    "        \n",
    "        print('CV: {:03d}, Epoch: {:03d}, Val Loss: {:.5f}, Val ACC: {:.5f},Val AUC: {:.5f}, Test ACC: {:.5f}, Test F1: {:.5f}, TEST SPE: {:.5f}, '\n",
    "                  'TEST SEN: {:.5f}, TEST AUC: {:.5f}'.format(n_fold +1, epoch + 1, v_loss, val_acc, val_auc, test_acc,test_f1,\n",
    "                                        test_spe,test_sen, test_auc))\n",
    "    \n",
    "   \n",
    "\n",
    "    eval_metrics2[n_fold, 0] = best_test_sen2\n",
    "    eval_metrics2[n_fold, 1] = best_test_spe2\n",
    "    eval_metrics2[n_fold, 2] = best_test_acc2\n",
    "    eval_metrics2[n_fold, 3] = best_test_f12\n",
    "    eval_metrics2[n_fold, 4] = best_test_auc2\n",
    "\n",
    "    eval_metrics3[n_fold, 0] = best_test_sen3\n",
    "    eval_metrics3[n_fold, 1] = best_test_spe3\n",
    "    eval_metrics3[n_fold, 2] = best_test_acc3\n",
    "    eval_metrics3[n_fold, 3] = best_test_f13\n",
    "    eval_metrics3[n_fold, 4] = best_test_auc3\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Corresponding to Best test_acc\")\n",
    "eval_df2 = pd.DataFrame(eval_metrics2)\n",
    "eval_df2.columns = ['SEN', 'SPE', 'ACC','F1', 'AUC-ROC']\n",
    "eval_df2.index = ['Fold_%02i' % (i + 1) for i in range(skf.n_splits)]\n",
    "print(eval_df2)\n",
    "print('Average Sensitivity: %.4f±%.4f' % (eval_metrics2[:, 0].mean(), eval_metrics2[:, 0].std()))\n",
    "print('Average Specificity: %.4f±%.4f' % (eval_metrics2[:, 1].mean(), eval_metrics2[:, 1].std()))\n",
    "print('Average Accuracy: %.4f±%.4f' % (eval_metrics2[:, 2].mean(), eval_metrics2[:, 2].std()))\n",
    "print('Average F1: %.4f±%.4f' % (eval_metrics2[:, 3].mean(), eval_metrics2[:, 3].std()))\n",
    "print('Average AUC-ROC: %.4f±%.4f' % (eval_metrics2[:, 4].mean(), eval_metrics2[:, 4].std()))\n",
    "\n",
    "print(\"\\nCorresponding to Maximum val_acc\")\n",
    "eval_df3 = pd.DataFrame(eval_metrics3)\n",
    "eval_df3.columns = ['SEN', 'SPE', 'ACC','F1', 'AUC-ROC']\n",
    "eval_df3.index = ['Fold_%02i' % (i + 1) for i in range(skf.n_splits)]\n",
    "print(eval_df3)\n",
    "print('Average Sensitivity: %.4f±%.4f' % (eval_metrics3[:, 0].mean(), eval_metrics3[:, 0].std()))\n",
    "print('Average Specificity: %.4f±%.4f' % (eval_metrics3[:, 1].mean(), eval_metrics3[:, 1].std()))\n",
    "print('Average Accuracy: %.4f±%.4f' % (eval_metrics3[:, 2].mean(), eval_metrics3[:, 2].std()))\n",
    "print('Average F1: %.4f±%.4f' % (eval_metrics3[:, 3].mean(), eval_metrics3[:, 3].std()))\n",
    "print('Average AUC-ROC: %.4f±%.4f' % (eval_metrics3[:, 4].mean(), eval_metrics3[:, 4].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradcam calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_cam(model, data):\n",
    "    data = data.to(device)\n",
    "    model.zero_grad()\n",
    "    data.x.requires_grad = True  \n",
    "\n",
    "    output, _ = model(data) \n",
    "\n",
    "    scores = F.softmax(output, dim=1)\n",
    "    print(scores.shape)\n",
    "    print(scores[0])\n",
    "\n",
    "    scores_mdd = scores[:, 1]          \n",
    "    print('scores = ',scores_mdd.shape)\n",
    "\n",
    "    scores_mdd.backward(torch.ones_like(scores_mdd))  \n",
    "\n",
    "    gradients = data.x.grad\n",
    "    print('gradients = ',gradients.shape)  \n",
    "\n",
    "    # Calculate Grad-CAM values \n",
    "    grad_cam = gradients * data.x\n",
    "    print('data.x = ',data.x.shape)  \n",
    "    print('gradcam = ',grad_cam.shape)  \n",
    "\n",
    "    contribution = torch.mean(grad_cam, dim=1)\n",
    "    \n",
    "    print('contribution = ',contribution.shape)  \n",
    "\n",
    "    return scores, grad_cam, contribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
